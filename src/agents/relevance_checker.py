from core.llm_client import LLMClient
import logging
logger = logging.getLogger(__name__)
class RelevanceChecker:
    def __init__(self):
        self.llm_client = LLMClient()
    def check(self, question: str, retriever, k=3) -> str:
        top_docs = retriever.invoke(question)
        if not top_docs:
            logger.debug("No documents returned from retriever.invoke(). Classifying as NO_MATCH.")
            return "NO_MATCH"
                # Combine the top k chunk texts into one string
        document_content = "\n\n".join(doc.page_content for doc in top_docs[:k])

        # Create a prompt for the LLM to classify relevance
        prompt = f"""
            You are an AI relevance checker between a user's question and provided document content.

            **Instructions:**
            - Classify how well the document content addresses the user's question.
            - Respond with only one of the following labels: CAN_ANSWER, PARTIAL, NO_MATCH.
            - Do not include any additional text or explanation.

            **Labels:**
            1) "CAN_ANSWER": The passages contain enough explicit information to fully answer the question.
            2) "PARTIAL": The passages mention or discuss the question's topic but do not provide all the details needed for a complete answer.
            3) "NO_MATCH": The passages do not discuss or mention the question's topic at all.

            **Important:** If the passages mention or reference the topic or timeframe of the question in any way, even if incomplete, respond with "PARTIAL" instead of "NO_MATCH".

            **Question:** {question}
            **Passages:** {document_content}

            **Respond ONLY with one of the following labels: CAN_ANSWER, PARTIAL, NO_MATCH**
            """    
        
        response = self.llm_client.invoke(prompt)
        llm_response = response.content.strip()
        valid_labels = {"CAN_ANSWER", "PARTIAL", "NO_MATCH"}
        if llm_response not in valid_labels:
            logger.debug("LLM did not respond with a valid label. Forcing 'NO_MATCH'.")
            classification = "NO_MATCH"
        else:
            logger.debug(f"Classification recognized as '{llm_response}'.")
            classification = llm_response

        return classification